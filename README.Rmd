---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  dpi = 300
)
```
# hvr

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
<!-- badges: end -->

The `hvr` package implements the harmony viewpoint regression model
from Peter M. C. Harrison's PhD thesis.
This model generates predictions for chord progressions on the basis of
an assortment of continuous and categorical perceptual features.

## Installation

From GitHub:

```r 
if (!requireNamespace("devtools")) install.packages("devtools")
devtools::install_github("pmcharrison/hvr")
```

## Example usage

```{r, include = FALSE, echo = TRUE}
if (!requireNamespace("hcorp"))
  stop("please install the hcorp package first: ", 
       "https://github.com/pmcharrison/hcorp")
library(hrep)
library(hvr)
```

```{r, cache = TRUE, include = TRUE, echo = TRUE, results = 'hide'}
x <- model_dataset(corpus_test = hcorp::popular_1[1],
                   corpus_pretrain = hcorp::popular_1[2:10],
                   output_dir = "tmp", 
                   viewpoints = hvr_viewpoints[c("root_pc", 
                                                 "hutch_78_roughness",
                                                 "spec_sim_3")],
                   poly_degree = 1)
```

```{r, include = TRUE, echo = TRUE, out.width = '60%'}
plot(x$pred$information_content,
     xlab = "Chord position",
     ylab = "Surprisal (bits)",
     type = "l")

```

```{r, include = TRUE, echo = TRUE, out.width = '60%'}
ggplot2::theme_set(ggplot2::theme_classic())
plot_perm_int(x$mod, error_bars = TRUE)
```

## Usage notes

### API

The top-level function is `model_dataset`, which automates the 
standard modelling pipeline for harmonic viewpoint regression.
This function wraps several lower-level subroutines
(e.g. viewpoint computation, PPM analyses)
which can be run individually if desired. 
See `?model_dataset` for more details.

### Training set sizes

Optimising feature weights is memory-intensive, and best done with
datasets of < 1,000 chords. Larger datasets can be modelled using
downsampling. 
Training categorical viewpoint models is relatively efficient;
the implementation can cope well with datasets of c. 100,000 chords.

## Future development

We have exciting ideas about how to improve the computational tractability,
robustness, and interpretability of the model.
We expect to be able to share an updated version by 2021,
but feel free to contact Peter Harrison
(pmc.harrison [at] gmail.com) before then for a progress report
and potentially a draft implementation.
